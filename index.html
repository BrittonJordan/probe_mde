<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="ProbeMDE: Uncertainty-Guided Active Proprioception for Monocular Depth Estimation in Surgical Robotics.">
  <meta name="keywords" content="ProbeMDE, monocular depth estimation, surgical robotics, active perception, uncertainty">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ProbeMDE: Uncertainty-Guided Active Proprioception for Monocular Depth Estimation in Surgical Robotics</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">ProbeMDE: Uncertainty-Guided Active Proprioception for Monocular Depth Estimation in Surgical Robotics</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://www.brittonty.com/">Britton Jordan</a><sup>*1</sup>,
            </span>
            <span class="author-block">
              <a href="https://jordan-lee-thompson.com/">Jordan Thompson</a><sup>*1</sup>,
            </span>
            <span class="author-block">
              Jesse F. d’Almeida<sup>2</sup>,
            </span>
            <span class="author-block">
              Hao Li<sup>2</sup>,
            </span>
            <span class="author-block">
              Nithesh Kumar<sup>2</sup>,
            </span>
            <span class="author-block">
              Susheela Sharma Stern<sup>2</sup>,
            </span>
            <span class="author-block">
              Ipek Oguz<sup>2</sup>,
            </span>
            <span class="author-block">
              Robert J. Webster III<sup>2</sup>,
            </span>
            <span class="author-block">
              Daniel Brown<sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://users.cs.utah.edu/~adk/">Alan Kuntz</a><sup>1</sup>,
            </span>
            <span class="author-block">
              James Ferguson<sup>1</sup>
            </span>
          </div>

    

        
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Utah,</span>
            <span class="author-block"><sup>2</sup>Vanderbilt University</span>
          </div>

          <div class="is-size-6 publication-authors">
            <span class="author-block"><sup>*</sup>These authors contributed equally.</span>
          </div>

          <!-- <div class="column has-text-centered">
            <div class="publication-links"> -->
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span> -->
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            <!-- </div> -->

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img id="probemde-teaser" src="./static/images/4b_probing_sequence.gif" alt="ProbeMDE teaser" style="height:100%; width:auto;" />
      <h2 class="subtitle has-text-centered">ProbeMDE uses cost-aware robot proprioception to refine a depth estimate of endoscopic imagery.</h2>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <!-- <h2 class="title is-3">Video</h2> -->
        <div class="publication-video">
          <video controls playsinline width="100%">
            <source src="./static/videos/ProbeMDE_paper_video_presentation_HD.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered" style="margin-top: 1rem;">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Monocular depth estimation (MDE) provides a useful tool for robotic perception, but its
            predictions are often uncertain and inaccurate in challenging environments such as surgical
            scenes where textureless surfaces, specular reflections, and occlusions are common. To address
            this, we propose ProbeMDE, a cost-aware active sensing framework that combines RGB images with
            sparse proprioceptive measurements for MDE. 
          </p>
          <p>

          Our approach utilizes an ensemble of MDE models to
          predict dense depth maps conditioned on both RGB images and on a sparse set of known depth
          measurements obtained via proprioception, where the robot has touched the environment in a known
          configuration. We quantify predictive uncertainty via the ensemble’s variance and measure the
          gradient of the uncertainty with respect to candidate measurement locations. The places with the highest negative values in our gradient map indicate the pixel locations where additional probes will yield the greatest reduction in ensemble variance.
          
          </p>
          <p>
            We validate our method in both
            simulated and physical experiments on central airway obstruction surgical phantoms. Our results
            demonstrate that our approach outperforms baseline methods across standard depth estimation
            metrics, achieving higher accuracy while minimizing the number of required proprioceptive
            measurements.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Motivation. -->
    <div class="columns is-centered has-text-centered" style="margin-top: 1rem;">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Motivation</h2>
        <div class="content has-text-justified">
          <p>
            Recent advancements in deep learning have led to large-scale foundation models for 
            monocular depth estimation that are highly performant on in-distribution cases. 
            However, these models perform poorly for endoscopic images where textureless surfaces, 
            specular reflections, and overall dissimilarity from training datasets make predictions difficult.

            Here we show Depth Anything V2’s predictions for both simulation and real endoscopic scenes.

          </p>
        </div>
        <figure class="image">
          <img src="./static/images/depth_anything_v2_comparison_simm.png" alt="Motivation figure"/>
        </figure>
        <figure class="image">
          <img src="./static/images/depth_anything_v2_comparison.png" alt="Motivation figure"/>
          <figcaption>Foundation models for MDE perform poorly on endoscopic images.</figcaption>
        </figure>
      </div>
    </div>
    <!--/ Motivation. -->

    <!-- Architecture. -->
    <div class="columns is-centered has-text-centered" style="margin-top: 1rem;">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Architecture</h2>
        <figure class="image">
          <img src="./static/images/ProbeMDE_architecture.jpg" alt="ProbeMDE architecture"/>
          <figcaption> ProbeMDE takes as input an endoscopic RGB image and a sparse propriocepted depth map. It produces a dense depth map and the best locations for future probes.</figcaption>
        </figure>
      </div>
    </div>
    <!--/ Architecture. -->

    <!-- Additional Results. -->
    <div class="columns is-centered has-text-centered" style="margin-top: 1rem;">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Recorded Demo</h2>
        <video id="probemde-live-sequence" controls muted loop playsinline height="100%">
          <source src="./static/videos/ProbeMDE_live_sequence_2_5x.mp4"
                  type="video/mp4">
        </video>
        <figure class="image" style="margin-top: 5rem;">
          <img src="./static/images/probing_sequence_2.gif" alt="Probing sequence"/>
        </figure>
      </div>
    </div>
    <!--/ Additional Results. -->

    <!-- Quantitative Results (full-width). -->
    <section class="section" style="padding-top: 0;">
      <div class="container is-fluid">
        <!-- Prevent column title wrapping -->
        <style>
          .quant-results table thead th,
          .quant-results table tbody td {
            white-space: nowrap;
          }
        </style>

        <div class="columns is-centered has-text-centered quant-results" style="margin-top: 1rem;">
          <div class="column is-12">
            <h2 class="title is-3">Quantitative Results</h2>

            <div class="content has-text-centered" style="overflow-x:auto;">
              <table class="table is-striped is-hoverable is-fullwidth">
                <thead>
                  <tr>
                    <th>Method</th>
                    <th>AbsRel</th>
                    <th>SqRel</th>
                    <th>RMSE</th>
                    <th>Log RMSE</th>
                    <th>ScInv</th>
                    <th>δ &lt; 1.25</th>
                    <th>δ &lt; 1.25^2</th>
                    <th>δ &lt; 1.25^3</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>No Sparse Ground-Truth</td><td>0.352</td><td>2.657</td><td>6.559</td><td>0.376</td><td>0.328</td><td>0.580</td><td>0.832</td><td>0.920</td>
                  </tr>
                  <tr>
                    <td>Random Selection</td><td>0.167</td><td>0.739</td><td>3.991</td><td>0.214</td><td>0.188</td><td>0.785</td><td>0.946</td><td>0.978</td>
                  </tr>
                  <tr>
                    <td>Greedy Variance</td><td>0.171</td><td>0.789</td><td>4.122</td><td>0.221</td><td>0.195</td><td>0.777</td><td>0.944</td><td>0.977</td>
                  </tr>
                  <tr>
                    <td>Stein Variance</td><td>0.170</td><td>0.789</td><td>4.182</td><td>0.221</td><td>0.195</td><td>0.776</td><td>0.942</td><td>0.976</td>
                  </tr>
                  <tr>
                    <td>Greedy Gradient</td><td>0.164</td><td>0.720</td><td><strong>3.878</strong></td><td>0.209</td><td>0.183</td><td>0.782</td><td>0.950</td><td><strong>0.981</strong></td>
                  </tr>
                  <tr>
                    <td>ProbeMDE (Ours)</td><td><strong>0.162</strong></td><td><strong>0.708</strong></td><td>3.920</td><td><strong>0.206</strong></td><td><strong>0.180</strong></td><td><strong>0.791</strong></td><td><strong>0.952</strong></td><td><strong>0.981</strong></td>
                  </tr>
                </tbody>
              </table>
              <p class="has-text-grey is-size-6">
                Performance comparison of active proprioception strategies on simulated data. Lower is better for AbsRel, SqRel, RMSE, Log RMSE, and ScInv. Higher is better for δ&lt;1.25^t. Best results in each column are bolded.
              </p>
            </div>

            <div class="content has-text-centered" style="overflow-x:auto; margin-top: 1rem;">
              <table class="table is-striped is-hoverable is-fullwidth">
                <thead>
                  <tr>
                    <th>Method</th>
                    <th>AbsRel</th>
                    <th>SqRel</th>
                    <th>RMSE</th>
                    <th>Log RMSE</th>
                    <th>ScInv</th>
                    <th>δ &lt; 1.25</th>
                    <th>δ &lt; 1.25^2</th>
                    <th>δ &lt; 1.25^3</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>Random Selection</td><td><strong>0.242</strong></td><td>3.745</td><td>14.254</td><td>0.391</td><td>0.331</td><td><strong>0.452</strong></td><td><strong>0.809</strong></td><td>0.939</td>
                  </tr>
                  <tr>
                    <td>Greedy Variance</td><td>0.261</td><td>3.695</td><td>13.797</td><td>0.385</td><td>0.316</td><td>0.377</td><td>0.774</td><td><strong>0.944</strong></td>
                  </tr>
                  <tr>
                    <td>Stein Variance</td><td>0.260</td><td>3.812</td><td>13.588</td><td>0.383</td><td>0.316</td><td>0.406</td><td>0.785</td><td>0.943</td>
                  </tr>
                  <tr>
                    <td>Greedy Gradient</td><td>0.264</td><td>4.021</td><td>14.503</td><td>0.410</td><td>0.341</td><td>0.392</td><td>0.767</td><td>0.936</td>
                  </tr>
                  <tr>
                    <td>ProbeMDE (Ours)</td><td>0.250</td><td><strong>3.682</strong></td><td><strong>13.582</strong></td><td><strong>0.380</strong></td><td><strong>0.315</strong></td><td>0.432</td><td>0.791</td><td>0.940</td>
                  </tr>
                </tbody>
              </table>
              <p class="has-text-grey is-size-6">
                Performance comparison of active proprioception strategies on physical data. Lower is better for AbsRel, SqRel, RMSE, Log RMSE, and ScInv. Higher is better for δ&lt;1.25^t. Best results in each column are bolded.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!--/ Quantitative Results. -->
  </div>
</section>





<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{jordan2025probemde,
  author    = {Jordan, Britton and Thompson, Jordan and d'Almeida, Jesse F. and Li, Hao and Kumar, Nithesh and Stern, Susheela Sharma and Oguz, Ipek and Webster III, Robert J. and Brown, Daniel and Kuntz, Alan and Ferguson, James},
  title     = {ProbeMDE: Uncertainty-Guided Active Proprioception for Monocular Depth Estimation in Surgical Robotics},
  journal   = {Preprint},
  year      = {2025},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
  
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <!-- <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p> -->
          <p>
            This website is based on the <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies template.</a>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
